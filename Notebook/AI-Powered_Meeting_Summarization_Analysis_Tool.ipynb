{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdWLJYn7APCB",
        "outputId": "35ab7148-5465-43e9-dbe2-6d57e0b8836d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git ffmpeg-python yt-dlp transformers sentencepiece scikit-learn weasyprint langid nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import whisper\n",
        "import langid\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from weasyprint import HTML\n",
        "from tqdm.auto import tqdm\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "67fbL82VAZza"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import os\n",
        "import subprocess\n",
        "from weasyprint import HTML\n",
        "\n",
        "# Global models dictionary\n",
        "_MODELS = {\n",
        "    \"whisper\": None,\n",
        "    \"summarizer\": None,\n",
        "    \"qa\": None,\n",
        "    \"sentiment\": None\n",
        "}\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"Load all models once with progress tracking\"\"\"\n",
        "    if not _MODELS[\"whisper\"]:\n",
        "        print(\"üîß Loading Whisper speech-to-text model...\")\n",
        "        _MODELS[\"whisper\"] = whisper.load_model(\"medium\")\n",
        "\n",
        "    if not _MODELS[\"summarizer\"]:\n",
        "        print(\"üîß Loading BART Summarizer...\")\n",
        "        _MODELS[\"summarizer\"] = pipeline(\"summarization\",\n",
        "                                      model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    if not _MODELS[\"qa\"]:\n",
        "        print(\"üîß Loading FLAN-T5 for Q&A...\")\n",
        "        _MODELS[\"qa\"] = pipeline(\"text2text-generation\",\n",
        "                             model=\"google/flan-t5-large\")\n",
        "\n",
        "    if not _MODELS[\"sentiment\"]:\n",
        "        print(\"üîß Loading Sentiment Analyzer...\")\n",
        "        _MODELS[\"sentiment\"] = pipeline(\"sentiment-analysis\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CXLZKSWXAkcW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeetingProcessor:\n",
        "    def __init__(self):\n",
        "        load_models()\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        self.stopwords = set(stopwords.words('english')).union({\n",
        "            'like', 'just', 'really', 'okay', 'um', 'uh', 'yeah'\n",
        "        })\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "        self.enable_advanced = False  # Advanced features off by default\n",
        "\n",
        "    def enable_advanced_features(self, enable=True):\n",
        "        \"\"\"Toggle advanced analysis features on/off\"\"\"\n",
        "        self.enable_advanced = enable\n",
        "\n",
        "    def youtube_to_text(self, url):\n",
        "        \"\"\"Convert YouTube video to cleaned transcript\"\"\"\n",
        "        print(\"üé• Processing YouTube video...\")\n",
        "\n",
        "        try:\n",
        "            # Download audio using subprocess\n",
        "            subprocess.run(\n",
        "                ['yt-dlp', '-f', 'bestaudio', url, '-o', 'audio_temp.mp4', '--quiet'],\n",
        "                check=True\n",
        "            )\n",
        "            subprocess.run(\n",
        "                ['ffmpeg', '-y', '-i', 'audio_temp.mp4', '-ar', '16000',\n",
        "                 '-ac', '1', '-c:a', 'pcm_s16le', 'audio_temp.wav', '-v', 'quiet'],\n",
        "                check=True\n",
        "            )\n",
        "\n",
        "            # Transcribe\n",
        "            result = _MODELS[\"whisper\"].transcribe(\"audio_temp.wav\")\n",
        "\n",
        "            # Clean up temporary files\n",
        "            if os.path.exists(\"audio_temp.mp4\"):\n",
        "                os.remove(\"audio_temp.mp4\")\n",
        "            if os.path.exists(\"audio_temp.wav\"):\n",
        "                os.remove(\"audio_temp.wav\")\n",
        "\n",
        "            # Merge segments into paragraphs\n",
        "            transcript = []\n",
        "            current_para = []\n",
        "            for seg in result['segments']:\n",
        "                if len(current_para) > 0 and seg['start'] - current_para[-1]['end'] > 2.0:\n",
        "                    transcript.append({\n",
        "                        'start': current_para[0]['start'],\n",
        "                        'end': current_para[-1]['end'],\n",
        "                        'text': ' '.join([s['text'] for s in current_para])\n",
        "                    })\n",
        "                    current_para = []\n",
        "                current_para.append(seg)\n",
        "\n",
        "            if current_para:\n",
        "                transcript.append({\n",
        "                    'start': current_para[0]['start'],\n",
        "                    'end': current_para[-1]['end'],\n",
        "                    'text': ' '.join([s['text'] for s in current_para])\n",
        "                })\n",
        "\n",
        "            return transcript\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing video: {e}\")\n",
        "            if os.path.exists(\"audio_temp.mp4\"):\n",
        "                os.remove(\"audio_temp.mp4\")\n",
        "            if os.path.exists(\"audio_temp.wav\"):\n",
        "                os.remove(\"audio_temp.wav\")\n",
        "            return []\n",
        "\n",
        "    def analyze_meeting(self, text, summary_level='detailed', style='bullet'):\n",
        "        \"\"\"Run analysis with optional advanced features\"\"\"\n",
        "        print(\"\\nüîç Analyzing content...\")\n",
        "\n",
        "        base_results = {\n",
        "            'summary': self._summarize(text, summary_level, style),\n",
        "            'keywords': self._extract_keywords(text),\n",
        "            'actions': self._extract_actions(text),\n",
        "            'sentiment': self._analyze_sentiment(text),\n",
        "        }\n",
        "\n",
        "        if self.enable_advanced:\n",
        "            print(\"üîß Running advanced analysis...\")\n",
        "            base_results.update({\n",
        "                'discussion_topics': self._extract_discussion_topics(text),\n",
        "                'decisions': self._extract_decisions(text),\n",
        "                'participant_sentiments': self._analyze_participant_sentiments(text),\n",
        "                'quotes': self._extract_quotes(text)\n",
        "            })\n",
        "\n",
        "        return base_results\n",
        "\n",
        "    # ===== ADVANCED ANALYSIS METHODS =====\n",
        "    def _extract_discussion_topics(self, text):\n",
        "        \"\"\"Extract main discussion topics from text\"\"\"\n",
        "        prompt = f\"\"\"Extract 3-5 main discussion topics from this meeting:\n",
        "                    {text[:3000]}... [truncated]\n",
        "                    Format as bullet points\"\"\"\n",
        "        return _MODELS[\"qa\"](prompt, max_length=200)[0]['generated_text']\n",
        "\n",
        "    def _extract_decisions(self, text):\n",
        "        \"\"\"Extract decisions made from text\"\"\"\n",
        "        prompt = f\"\"\"List key decisions made in this meeting:\n",
        "                    {text[:3000]}... [truncated]\n",
        "                    Format as bullet points\"\"\"\n",
        "        return _MODELS[\"qa\"](prompt, max_length=200)[0]['generated_text']\n",
        "\n",
        "    def _analyze_participant_sentiments(self, text):\n",
        "        \"\"\"Analyze sentiment by participant\"\"\"\n",
        "        prompt = f\"\"\"Analyze participant sentiments from:\n",
        "                    {text[:3000]}... [truncated]\n",
        "                    Format as: [Name]: [Sentiment] [Confidence%]\"\"\"\n",
        "        analysis = _MODELS[\"qa\"](prompt, max_length=400)[0]['generated_text']\n",
        "\n",
        "        participants = {}\n",
        "        for line in analysis.splitlines():\n",
        "            if ':' in line and '%' in line:\n",
        "                try:\n",
        "                    name, rest = line.split(':', 1)\n",
        "                    sentiment, confidence = rest.strip().rsplit(' ', 1)\n",
        "                    participants[name.strip()] = (\n",
        "                        sentiment.strip(),\n",
        "                        float(confidence.strip('%'))/100\n",
        "                    )\n",
        "                except:\n",
        "                    continue\n",
        "        return participants\n",
        "\n",
        "    def _extract_quotes(self, text):\n",
        "        \"\"\"Extract notable quotes from the meeting\"\"\"\n",
        "        prompt = f\"\"\"Extract 3-5 notable quotes from this meeting:\n",
        "                    {text[:3000]}... [truncated]\n",
        "                    Format one quote per line\"\"\"\n",
        "        quotes = _MODELS[\"qa\"](prompt, max_length=300)[0]['generated_text']\n",
        "        return [q.strip() for q in quotes.splitlines() if q.strip()]\n",
        "\n",
        "    # ===== CORE ANALYSIS METHODS =====\n",
        "    def _summarize(self, text, mode, style):\n",
        "        \"\"\"Generate formatted summary with dynamic length calculation\"\"\"\n",
        "        chunks = self._chunk_text(text)\n",
        "        summaries = []\n",
        "\n",
        "        for chunk in tqdm(chunks, desc=f\"Summarizing ({mode})\"):\n",
        "            input_length = len(self.tokenizer(chunk)['input_ids'])\n",
        "\n",
        "            if mode == 'short':\n",
        "                max_len = max(30, int(input_length * 0.3))\n",
        "                min_len = max(10, int(input_length * 0.15))\n",
        "            elif mode == 'medium':\n",
        "                max_len = max(60, int(input_length * 0.5))\n",
        "                min_len = max(30, int(input_length * 0.25))\n",
        "            else:  # detailed\n",
        "                max_len = max(100, int(input_length * 0.7))\n",
        "                min_len = max(50, int(input_length * 0.4))\n",
        "\n",
        "            summary = _MODELS[\"summarizer\"](\n",
        "                chunk,\n",
        "                max_length=max_len,\n",
        "                min_length=min_len,\n",
        "                do_sample=False\n",
        "            )[0]['summary_text']\n",
        "            summaries.append(summary)\n",
        "\n",
        "        # Final summary\n",
        "        final = _MODELS[\"summarizer\"](\n",
        "            ' '.join(summaries),\n",
        "            max_length=max(50, int(len(' '.join(summaries).split())*0.6)),\n",
        "            min_length=max(25, int(len(' '.join(summaries).split())*0.3)),\n",
        "            do_sample=False\n",
        "        )[0]['summary_text']\n",
        "\n",
        "        return self._format_summary(final, style)\n",
        "\n",
        "    def _format_summary(self, text, style):\n",
        "        \"\"\"Convert to requested format with fixed numbering\"\"\"\n",
        "        # First split into sentences\n",
        "        sentences = [s.strip() for s in re.split(r'(?<=[.!?]) +', text) if s.strip()]\n",
        "\n",
        "        # Clean each sentence by removing any existing numbering\n",
        "        cleaned_sentences = []\n",
        "        for s in sentences:\n",
        "            # Remove any existing numbering patterns (1., 2. etc.)\n",
        "            cleaned = re.sub(r'^\\d+\\.\\s*', '', s)\n",
        "            # Remove any bullet points or other markers\n",
        "            cleaned = re.sub(r'^[‚Ä¢‚ô¶‚Ä∫‚û¢‚úì]\\s*', '', cleaned)\n",
        "            cleaned_sentences.append(cleaned.strip())\n",
        "\n",
        "        # Apply the requested formatting style\n",
        "        if style == 'bullet':\n",
        "            return '\\n'.join(f'‚Ä¢ {s}' for s in cleaned_sentences)\n",
        "        elif style == 'numbered':\n",
        "            return '\\n'.join(f'{i+1}. {s}' for i, s in enumerate(cleaned_sentences))\n",
        "        elif style == 'highlight':\n",
        "            return '\\n'.join(f'‚ú® {s} ‚ú®' for s in cleaned_sentences)\n",
        "        elif style == 'executive':\n",
        "            return 'KEY TAKEAWAYS:\\n' + '\\n'.join(f'‚úì {s}' for s in cleaned_sentences)\n",
        "        return '\\n'.join(cleaned_sentences)\n",
        "\n",
        "    def _extract_actions(self, text):\n",
        "        \"\"\"Identify action items with owners\"\"\"\n",
        "        prompt = f\"\"\"Extract action items from:\n",
        "                    {text[:3000]}... [truncated]\n",
        "                    Format as: - [Owner] [Action] [Deadline]\"\"\"\n",
        "        return _MODELS[\"qa\"](prompt, max_length=300)[0]['generated_text']\n",
        "\n",
        "    def _extract_keywords(self, text, top_n=15):\n",
        "        \"\"\"Get important terms with TF-IDF\"\"\"\n",
        "        vectorizer = TfidfVectorizer(stop_words=list(self.stopwords),\n",
        "                                   ngram_range=(1, 2))\n",
        "        X = vectorizer.fit_transform([text])\n",
        "        return [vectorizer.get_feature_names_out()[i]\n",
        "               for i in X.toarray()[0].argsort()[-top_n:][::-1]]\n",
        "\n",
        "    def _analyze_sentiment(self, text):\n",
        "        \"\"\"Enhanced sentiment analysis\"\"\"\n",
        "        result = _MODELS[\"sentiment\"](text[:1000])[0]\n",
        "        pos_score = result['score'] if result['label'] == 'POSITIVE' else 1 - result['score']\n",
        "        return ('NEUTRAL', pos_score) if abs(pos_score - 0.5) < 0.15 else (result['label'], result['score'])\n",
        "\n",
        "    def _chunk_text(self, text, chunk_size=400):\n",
        "        \"\"\"Split text preserving sentence boundaries\"\"\"\n",
        "        sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            words = sent.split()\n",
        "            if current_length + len(words) > chunk_size and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "            current_chunk.append(sent)\n",
        "            current_length += len(words)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "        return chunks\n",
        "\n"
      ],
      "metadata": {
        "id": "-7c2UaZtA2Cv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pdf_report(results, transcript=\"\", output_file=\"meeting_report.pdf\",\n",
        "                      summary_level=\"detailed\", style=\"bullet\"):\n",
        "    \"\"\"\n",
        "    Generate adaptive PDF report with:\n",
        "    - Blue title styling\n",
        "    - Fixed numbering issues\n",
        "    - Dynamic sections based on available data\n",
        "    \"\"\"\n",
        "    sentiment_color = {\n",
        "        'POSITIVE': '#2e7d32',\n",
        "        'NEGATIVE': '#c62828',\n",
        "        'NEUTRAL': '#f9a825'\n",
        "    }.get(results['sentiment'][0], '#777')\n",
        "\n",
        "    # Format action items\n",
        "    action_items = [line.strip() for line in results['actions'].splitlines()\n",
        "                   if line.strip() and not line.startswith(\"[Owner]\")]\n",
        "\n",
        "    # Format summary content\n",
        "    def format_content(text, content_style):\n",
        "        lines = [re.sub(r'^\\d+\\.\\s*', '', line.strip())  # Remove existing numbers\n",
        "                for line in text.splitlines() if line.strip()]\n",
        "        if content_style == 'numbered':\n",
        "            return \"<br>\".join(f\"{i+1}. {line}\" for i, line in enumerate(lines))\n",
        "        elif content_style == 'bullet':\n",
        "            return \"<br>\".join(f\"‚Ä¢ {line}\" for line in lines)\n",
        "        elif content_style == 'highlight':\n",
        "            return \"<br>\".join(f\"‚ú® {line} ‚ú®\" for line in lines)\n",
        "        elif content_style == 'executive':\n",
        "            return \"<br>\".join(f\"‚úì {line}\" for line in lines)\n",
        "        return \"<br>\".join(lines)\n",
        "\n",
        "    # Generate dynamic sections\n",
        "    advanced_sections = \"\"\n",
        "    if 'discussion_topics' in results:\n",
        "        advanced_sections += f\"\"\"\n",
        "        <h2>Discussion Topics</h2>\n",
        "        <div class=\"section\">\n",
        "            {format_content(results['discussion_topics'], 'bullet')}\n",
        "        </div>\"\"\"\n",
        "\n",
        "    if 'decisions' in results:\n",
        "        advanced_sections += f\"\"\"\n",
        "        <h2>Key Decisions</h2>\n",
        "        <div class=\"section\">\n",
        "            {format_content(results['decisions'], 'bullet')}\n",
        "        </div>\"\"\"\n",
        "\n",
        "    if 'participant_sentiments' in results:\n",
        "        advanced_sections += f\"\"\"\n",
        "        <h2>Participant Sentiments</h2>\n",
        "        <div class=\"section\">\n",
        "            {format_participant_sentiments(results['participant_sentiments'])}\n",
        "        </div>\"\"\"\n",
        "\n",
        "    if 'quotes' in results and results['quotes']:\n",
        "        advanced_sections += f\"\"\"\n",
        "        <h2>Highlighted Quotes</h2>\n",
        "        <div class=\"section\">\n",
        "            {\"\".join(f'<div class=\"quote\">\"{q}\"</div>' for q in results['quotes'])}\n",
        "        </div>\"\"\"\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <meta charset=\"utf-8\">\n",
        "        <style>\n",
        "            body {{\n",
        "                font-family: 'Segoe UI', sans-serif;\n",
        "                margin: 30px;\n",
        "                color: #333;\n",
        "                line-height: 1.6;\n",
        "            }}\n",
        "            h1 {{\n",
        "                color: #1a73e8;\n",
        "                border-bottom: 2px solid #1a73e8;\n",
        "                padding-bottom: 10px;\n",
        "            }}\n",
        "            h2 {{\n",
        "                color: #1a73e8;\n",
        "                margin-top: 25px;\n",
        "                border-left: 4px solid #1a73e8;\n",
        "                padding-left: 10px;\n",
        "            }}\n",
        "            .section {{\n",
        "                background: #f8f9fa;\n",
        "                padding: 15px 20px;\n",
        "                border-radius: 6px;\n",
        "                margin-bottom: 20px;\n",
        "            }}\n",
        "            .badge {{\n",
        "                background: {sentiment_color};\n",
        "                color: white;\n",
        "                padding: 3px 10px;\n",
        "                border-radius: 12px;\n",
        "                font-weight: bold;\n",
        "                display: inline-block;\n",
        "            }}\n",
        "            .keywords span {{\n",
        "                background: #e8f0fe;\n",
        "                color: #1a73e8;\n",
        "                padding: 4px 12px;\n",
        "                border-radius: 16px;\n",
        "                display: inline-block;\n",
        "                margin: 4px;\n",
        "                font-size: 0.9em;\n",
        "            }}\n",
        "            .transcript {{\n",
        "                font-family: monospace;\n",
        "                background: #f1f3f4;\n",
        "                padding: 15px;\n",
        "                border-radius: 5px;\n",
        "                white-space: pre-wrap;\n",
        "                line-height: 1.4;\n",
        "            }}\n",
        "            .quote {{\n",
        "                font-style: italic;\n",
        "                color: #555;\n",
        "                border-left: 3px solid #6fa8dc;\n",
        "                padding-left: 15px;\n",
        "                margin: 10px 0;\n",
        "            }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>ü§ñ Meeting Intelligence Report</h1>\n",
        "        <p><strong>Overall Sentiment:</strong> <span class=\"badge\">{results['sentiment'][0]} ({results['sentiment'][1]:.0%})</span></p>\n",
        "\n",
        "        <h2>Meeting Summary</h2>\n",
        "        <div class=\"section\">\n",
        "            {format_content(results['summary'], style)}\n",
        "        </div>\n",
        "\n",
        "        {advanced_sections}\n",
        "\n",
        "        <h2>Action Items</h2>\n",
        "        <div class=\"section\">\n",
        "            <ol>{\"\".join(f\"<li>{item}</li>\" for item in action_items)}</ol>\n",
        "        </div>\n",
        "\n",
        "        <h2>Key Terms</h2>\n",
        "        <div class=\"section keywords\">\n",
        "            {\" \".join(f\"<span>{kw}</span>\" for kw in results['keywords'])}\n",
        "        </div>\n",
        "\n",
        "        {f'<h2>Complete Transcript</h2><div class=\"transcript\">{transcript}</div>' if transcript else ''}\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    HTML(string=html).write_pdf(output_file)\n",
        "    print(f\"‚úÖ Report saved as: {output_file}\")\n",
        "\n",
        "def format_participant_sentiments(sentiments):\n",
        "    \"\"\"Format participant sentiment analysis\"\"\"\n",
        "    if not sentiments:\n",
        "        return \"<p>No participant-level analysis available.</p>\"\n",
        "\n",
        "    items = []\n",
        "    for name, (sentiment, score) in sentiments.items():\n",
        "        color = {\n",
        "            'POSITIVE': '#2e7d32',\n",
        "            'NEGATIVE': '#c62828',\n",
        "            'NEUTRAL': '#f9a825'\n",
        "        }.get(sentiment, '#777')\n",
        "        items.append(f\"\"\"\n",
        "        <div style=\"margin-bottom: 8px;\">\n",
        "            <strong>{name}:</strong>\n",
        "            <span style=\"background: {color}; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.9em;\">\n",
        "                {sentiment} ({score:.0%})\n",
        "            </span>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "    return \"\".join(items)\n",
        "\n",
        "def generate_transcript_pdf(transcript, output_file=\"meeting_transcript.pdf\"):\n",
        "    \"\"\"Generate transcript-only PDF\"\"\"\n",
        "    html = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <meta charset=\"utf-8\">\n",
        "        <style>\n",
        "            body {{\n",
        "                font-family: monospace;\n",
        "                margin: 30px;\n",
        "                color: #333;\n",
        "                background: #fff;\n",
        "                white-space: pre-wrap;\n",
        "                line-height: 1.4;\n",
        "            }}\n",
        "            h1 {{\n",
        "                color: #1a73e8;\n",
        "                border-bottom: 2px solid #1a73e8;\n",
        "                padding-bottom: 10px;\n",
        "            }}\n",
        "            .transcript {{\n",
        "                background: #f1f3f4;\n",
        "                padding: 15px;\n",
        "                border-radius: 5px;\n",
        "                margin-top: 20px;\n",
        "            }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>üìù Complete Transcript</h1>\n",
        "        <div class=\"transcript\">{transcript}</div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    HTML(string=html).write_pdf(output_file)\n",
        "    print(f\"üìÑ Transcript saved as: {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6eH0XcYiy5KP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community sentence-transformers faiss-cpu huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58fOp5lOb4NU",
        "outputId": "03b98ac9-7ddb-42ab-bbcc-e407baefae96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.71)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = MeetingProcessor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCUQ6xiIHMRD",
        "outputId": "0d4186ba-7724-4e9e-bdfd-da01f97183ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Loading Whisper speech-to-text model...\n",
            "üîß Loading BART Summarizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Loading FLAN-T5 for Q&A...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Loading Sentiment Analyzer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "#Rag Pipeline\n",
        "# Extract transcript text from MeetingProcessor.youtube_to_text\n",
        "transcript_data = processor.youtube_to_text(\"https://www.youtube.com/watch?v=U5HvuKEjH6g\")\n",
        "\n",
        "# Join all segments into one long string\n",
        "full_text = \" \".join(seg[\"text\"] for seg in transcript_data if \"text\" in seg)\n",
        "\n",
        "# Use the full_text variable generated by the youtube_to_text function\n",
        "documents = [type('obj', (object,), {'page_content': full_text, 'metadata': {'source': 'transcript'}})()]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(f\"üìù Document split into {len(docs)} chunks.\")\n",
        "\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "print(f\"üîé Loading embedding model: {embedding_model_name}\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "print(\"üì¶ Creating vector store...\")\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "print(\"‚úÖ Vector store is ready.\")\n",
        "\n",
        "model_id = \"google/flan-t5-base\"\n",
        "print(f\"ü§ñ Loading language model: {model_id} (this may take a moment...)\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    repetition_penalty=1.2,\n",
        "    no_repeat_ngram_size=3,\n",
        "    max_new_tokens=256,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "print(\"üöÄ RAG system is fully initialized and ready to answer questions.\")\n",
        "\n",
        "try:\n",
        "    print(\"\\n--- Testing RAG System ---\")\n",
        "    queries = [\n",
        "        \"What is Jack Ma‚Äôs main advice about spending time wisely?\",\n",
        "        \"What mistakes does Jack Ma recommend avoiding while you're young\",\n",
        "        \"What does Jack Ma say about competition and failure?\",\n",
        "        \"How does Jack Ma recommend balancing work and personal development?\"\n",
        "    ]\n",
        "    for query in queries:\n",
        "        result = qa_chain.invoke({\"query\": query})\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"Answer:\", result[\"result\"])\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå An error occurred while querying the RAG chain: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TSVIhENbD6p",
        "outputId": "0e989f58-1ba3-4142-e66d-c48a44e08354"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé• Processing YouTube video...\n",
            "üìù Document split into 19 chunks.\n",
            "üîé Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8-2771981323.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Creating vector store...\n",
            "‚úÖ Vector store is ready.\n",
            "ü§ñ Loading language model: google/flan-t5-base (this may take a moment...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/tmp/ipython-input-8-2771981323.py:44: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ RAG system is fully initialized and ready to answer questions.\n",
            "\n",
            "--- Testing RAG System ---\n",
            "\n",
            "Query: What is Jack Ma‚Äôs main advice about spending time wisely?\n",
            "Answer: He says, if you don't get better, you will be stuck.\n",
            "\n",
            "Query: What mistakes does Jack Ma recommend avoiding while you're young\n",
            "Answer: No money, no connections, no good grades\n",
            "\n",
            "Query: What does Jack Ma say about competition and failure?\n",
            "Answer: Jack Ma says competition is not about giant leaps. It's about small steps taken every day.\n",
            "\n",
            "Query: How does Jack Ma recommend balancing work and personal development?\n",
            "Answer: Helpful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyTelegramBotAPI openai moviepy #put it up with others when you finish\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUsEB9umExki",
        "outputId": "3865f932-36cd-44bd-dcc8-af8caaadf635"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyTelegramBotAPI in /usr/local/lib/python3.11/dist-packages (4.28.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from pyTelegramBotAPI) (3.12.14)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from pyTelegramBotAPI) (8.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pyTelegramBotAPI) (2.32.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from pyTelegramBotAPI) (0.45.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pyTelegramBotAPI) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pyTelegramBotAPI) (2.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->pyTelegramBotAPI) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->pyTelegramBotAPI) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->pyTelegramBotAPI) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->pyTelegramBotAPI) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->pyTelegramBotAPI) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->pyTelegramBotAPI) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->pyTelegramBotAPI) (1.20.1)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest->pyTelegramBotAPI) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from pytest->pyTelegramBotAPI) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->pyTelegramBotAPI) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest->pyTelegramBotAPI) (2.19.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_file_upload(message):\n",
        "    transcript_paragraphs = processor.youtube_to_text(video_url)\n",
        "    full_text = '\\n\\n'.join([f\"[{para['start']:.1f}s] {para['text']}\" for para in transcript_paragraphs])\n",
        "    re"
      ],
      "metadata": {
        "id": "qDZi5x1ZMwzM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot\n",
        "import os\n",
        "from moviepy.editor import VideoFileClip\n",
        "from telebot import types\n",
        "import subprocess\n",
        "import uuid\n",
        "from yt_dlp import YoutubeDL\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import torch\n",
        "\n",
        "API_TOKEN = '7030233167:AAGDX6fAbSva8vPCa9t-5GmMvDAmA_0NuS8'\n",
        "bot = telebot.TeleBot(API_TOKEN)\n",
        "chat_id_to_transcript = {}\n",
        "chat_rag_sessions = {}\n",
        "# Step 1: Welcome and Options\n",
        "@bot.message_handler(commands=['start'])\n",
        "def send_welcome(message):\n",
        "    chat_id = message.chat.id\n",
        "    bot.reply_to(message, (\n",
        "        \"üéâ Welcome to the Meeting Analysis Bot!\\n\\n\"\n",
        "        \"This bot will help you wrap up your meetings by providing transcription, summarization, and Q&A.\\n\\n\"\n",
        "        \"Would you like to send a YouTube link or upload a file?\\n\\n\"\n",
        "        \"Please reply with:\\n\"\n",
        "        \"`youtube` - to send a YouTube video link\\n\"\n",
        "        \"`file` - to upload a video file\"\n",
        "    ), parse_mode='Markdown')\n",
        "\n",
        "    bot.register_next_step_handler(message, handle_input_choice)\n",
        "\n",
        "\n",
        "def handle_input_choice(message):\n",
        "    choice = message.text.strip().lower()\n",
        "\n",
        "    if choice == 'youtube' or choice == 'Youtube':\n",
        "        bot.register_next_step_handler(message, handle_youtube_option)\n",
        "    elif choice == 'file' or choice == 'File':\n",
        "        bot.register_next_step_handler(message, handle_file_upload_option)\n",
        "    else:\n",
        "        bot.send_message(message.chat.id, \"‚ùå Invalid choice. Please reply with `youtube` or `file`.\")\n",
        "        bot.register_next_step_handler(message, handle_input_choice)\n",
        "\n",
        "\n",
        "# Step 2: Handle \"Upload Video\" Option\n",
        "@bot.message_handler(func=lambda msg: msg.text == \"File\")\n",
        "def handle_file_upload_option(message):\n",
        "    bot.send_message(message.chat.id, \"üì§ Great! Please upload the video file now.\")\n",
        "\n",
        "# Step 3: Handle video or document upload\n",
        "@bot.message_handler(content_types=['video', 'document'])\n",
        "def handle_media(message):\n",
        "    try:\n",
        "        file_id = message.video.file_id if message.content_type == 'video' else message.document.file_id\n",
        "        file_info = bot.get_file(file_id)\n",
        "        downloaded_file = bot.download_file(file_info.file_path)\n",
        "\n",
        "        os.makedirs(\"temp\", exist_ok=True)\n",
        "        input_path = f\"temp/{file_info.file_unique_id}.mp4\"\n",
        "        with open(input_path, 'wb') as f:\n",
        "            f.write(downloaded_file)\n",
        "\n",
        "        bot.reply_to(message, \"üîÑ Converting video to audio...\")\n",
        "\n",
        "        # Convert to WAV\n",
        "        clip = VideoFileClip(input_path)\n",
        "        audio_path = input_path.replace(\".mp4\", \".wav\")\n",
        "        clip.audio.write_audiofile(audio_path)\n",
        "        clip.close()\n",
        "\n",
        "        bot.reply_to(message, \"üß† Transcribing meeting audio please wait (this will take some minutes)...\")\n",
        "\n",
        "        # Transcribe with Whisper\n",
        "        result = _MODELS['whisper'].transcribe(audio_path)\n",
        "        full_text = '\\n\\n'.join([f\"[{para['start']:.1f}s] {para['text']}\" for para in result])\n",
        "        transcript = result[\"text\"]\n",
        "\n",
        "        # Clean up\n",
        "        os.remove(input_path)\n",
        "        os.remove(audio_path)\n",
        "\n",
        "\n",
        "        bot.reply_to(message, f\"‚úÖ Transcription complete!\")\n",
        "        chat_id_to_transcript[message.chat.id] = transcript  # Save transcript for later\n",
        "        start_config(message)  # Start the interactive config steps\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.reply_to(message, f\"‚ùå An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: When user selects \"Send YouTube Link\"\n",
        "@bot.message_handler(func=lambda msg: msg.text == \"YouTube\")\n",
        "def handle_youtube_option(message):\n",
        "    bot.send_message(message.chat.id, \"üîó Please send the YouTube video link now.\")\n",
        "    bot.register_next_step_handler(message, process_youtube_link)\n",
        "\n",
        "# Step 2: Process the link\n",
        "def process_youtube_link(message):\n",
        "    url = message.text.strip()\n",
        "    chat_id = message.chat.id\n",
        "\n",
        "    if not url.startswith(\"http\"):\n",
        "        bot.send_message(chat_id, \"‚ùå That doesn‚Äôt look like a valid link. Please try again.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        bot.send_message(chat_id, \"üß† Transcribing meeting audio please wait (this will take some minutes)...\")\n",
        "\n",
        "        transcript_paragraphs = processor.youtube_to_text(url)\n",
        "        full_text = '\\n\\n'.join([f\"[{para['start']:.1f}s] {para['text']}\" for para in transcript_paragraphs])\n",
        "\n",
        "\n",
        "        bot.send_message(chat_id, f\"‚úÖ Transcription complete!\")\n",
        "        chat_id_to_transcript[message.chat.id] = full_text  # Save transcript for later\n",
        "        start_config(message)  # Start the interactive config steps\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.send_message(chat_id, f\"‚ùå An error occurred: {str(e)}\")\n",
        "\n",
        "user_config = {}\n",
        "\n",
        "@bot.message_handler(func=lambda m: m.text.startswith(\"‚úÖ Transcription complete\"))\n",
        "def start_config(message):\n",
        "    chat_id = message.chat.id\n",
        "    user_config[chat_id] = {\"transcript\":chat_id_to_transcript.get(chat_id, \"\")}  # Save transcript for later\n",
        "\n",
        "    bot.send_message(chat_id, \"Enable advanced analysis, ex: like summarization? (yes/no) \")\n",
        "    bot.register_next_step_handler(message, handle_advanced_analysis)\n",
        "\n",
        "def handle_advanced_analysis(message):\n",
        "    chat_id = message.chat.id\n",
        "    user_config[chat_id][\"advanced\"] = message.text.strip().lower() == \"yes\"\n",
        "\n",
        "    bot.send_message(chat_id, \"üìù Summary Preferences:\\nLevel (short/medium/detailed)\")\n",
        "    bot.register_next_step_handler(message, handle_summary_level)\n",
        "\n",
        "def handle_summary_level(message):\n",
        "    chat_id = message.chat.id\n",
        "    level = message.text.strip().lower()\n",
        "    user_config[chat_id][\"summary_level\"] = level if level else \"detailed\"\n",
        "\n",
        "    bot.send_message(chat_id, \"Style (bullet/numbered/executive/highlight)\")\n",
        "    bot.register_next_step_handler(message, handle_style)\n",
        "\n",
        "def handle_style(message):\n",
        "    chat_id = message.chat.id\n",
        "    style = message.text.strip().lower()\n",
        "    user_config[chat_id][\"style\"] = style if style else \"bullet\"\n",
        "\n",
        "    bot.send_message(chat_id, \"Include full transcript? (yes/no)\")\n",
        "    bot.register_next_step_handler(message, handle_include_transcript)\n",
        "\n",
        "def handle_include_transcript(message):\n",
        "    chat_id = message.chat.id\n",
        "    user_config[chat_id][\"include_transcript\"] = message.text.strip().lower() == \"yes\"\n",
        "\n",
        "    bot.send_message(chat_id, \"Output filename (without extension)\")\n",
        "    bot.register_next_step_handler(message, handle_filename)\n",
        "\n",
        "def handle_filename(message):\n",
        "    chat_id = message.chat.id\n",
        "    filename = message.text.strip() or \"meeting_report\"\n",
        "    user_config[chat_id][\"output_name\"] = filename\n",
        "\n",
        "    if not user_config[chat_id][\"include_transcript\"]:\n",
        "        bot.send_message(chat_id, \"Save transcript as separate file? (yes/no) \")\n",
        "        bot.register_next_step_handler(message, handle_separate_transcript)\n",
        "    else:\n",
        "        finalize_analysis(message)\n",
        "\n",
        "def handle_separate_transcript(message):\n",
        "    chat_id = message.chat.id\n",
        "    user_config[chat_id][\"separate_transcript\"] = message.text.strip().lower() == \"yes\"\n",
        "    finalize_analysis(message)\n",
        "\n",
        "def finalize_analysis(message):\n",
        "    chat_id = message.chat.id\n",
        "    config = user_config[chat_id]\n",
        "\n",
        "    # üß† Here you would call your processing pipeline:\n",
        "    results = processor.analyze_meeting(\n",
        "        config[\"transcript\"],\n",
        "        config[\"summary_level\"],\n",
        "        config[\"style\"]\n",
        "    )\n",
        "\n",
        "    if config[\"include_transcript\"]:\n",
        "        bot.send_message(chat_id, \"Generating your pdf...\")\n",
        "        output_file = f\"{config['output_name']}.pdf\"  # ‚úÖ Define this\n",
        "\n",
        "        generate_pdf_report(\n",
        "            results,\n",
        "            transcript=config[\"transcript\"],\n",
        "            output_file=f\"{config['output_name']}_full.pdf\",\n",
        "            summary_level=config[\"summary_level\"],\n",
        "            style=config[\"style\"]\n",
        "        )\n",
        "    else:\n",
        "        bot.send_message(chat_id, \"Generating your pdf...\")\n",
        "        output_file = f\"{config['output_name']}.pdf\"  # ‚úÖ Define this\n",
        "\n",
        "        generate_pdf_report(\n",
        "            results,\n",
        "            output_file=f\"{config['output_name']}.pdf\",\n",
        "            summary_level=config[\"summary_level\"],\n",
        "            style=config[\"style\"]\n",
        "        )\n",
        "        if config.get(\"separate_transcript\"):\n",
        "            generate_transcript_pdf(config[\"transcript\"], f\"{config['output_name']}_transcript.pdf\")\n",
        "\n",
        "    bot.send_message(chat_id, \"‚úÖ Analysis complete! Report is generated.\")\n",
        "    with open(output_file, \"rb\") as pdf_file:\n",
        "      bot.send_document(chat_id, pdf_file)\n",
        "    initialize_rag_pipeline(message)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def initialize_rag_pipeline(message):\n",
        "    try:\n",
        "        chat_id = message.chat.id\n",
        "        bot.send_message(chat_id,\"\\nüß† Initializing RAG pipeline...\")\n",
        "\n",
        "        # Step 1: Document creation and splitting\n",
        "        chat_id = message.chat.id\n",
        "        transcript_text = chat_id_to_transcript.get(chat_id, \"\")\n",
        "        documents = [type('Doc', (), {'page_content': transcript_text, 'metadata': {'source': 'transcript'}})()]\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "        docs = text_splitter.split_documents(documents)\n",
        "        print(f\"üìù Document split into {len(docs)} chunks.\")\n",
        "\n",
        "        # Step 2: Embedding model and FAISS vector store\n",
        "        embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        print(f\"üîé Loading embedding model: {embedding_model_name}\")\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "        db = FAISS.from_documents(docs, embeddings)\n",
        "        print(\"‚úÖ Vector store is ready.\")\n",
        "\n",
        "        # Step 3: Load the LLM pipeline\n",
        "        model_id = \"google/flan-t5-base\"\n",
        "        bot.send_message(chat_id,f\"ü§ñ Loading language model: {model_id}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        hf_pipe = pipeline(\n",
        "            \"text2text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=256,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
        "\n",
        "        # Step 4: Create RAG chain\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=db.as_retriever(),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        bot.send_message(chat_id,\"üöÄ RAG system is fully initialized.\")\n",
        "        bot.send_message(chat_id,\"Send ask_command if you have any questions regarding the meeting.\")\n",
        "        chat_rag_sessions[chat_id] = qa_chain\n",
        "        return qa_chain\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.send_message(chat_id,f\"\\n‚ùå An error occurred while initializing RAG: {e}\")\n",
        "        return None\n",
        "\n",
        "@bot.message_handler(commands=['ask'])\n",
        "def handle_question(message):\n",
        "    chat_id = message.chat.id\n",
        "    qa_chain = chat_rag_sessions.get(chat_id)\n",
        "\n",
        "    if not qa_chain:\n",
        "        bot.reply_to(message, \"‚ùóÔ∏èPlease analyze a meeting first before asking questions.\")\n",
        "        return\n",
        "\n",
        "    bot.reply_to(message, \"üí¨ Ask your question about the meeting:\")\n",
        "    bot.register_next_step_handler(message, process_user_question)\n",
        "\n",
        "\n",
        "def process_user_question(message):\n",
        "    chat_id = message.chat.id\n",
        "    question = message.text\n",
        "    qa_chain = chat_rag_sessions.get(chat_id)\n",
        "\n",
        "    if not qa_chain:\n",
        "        bot.reply_to(message, \"‚ö†Ô∏è RAG session not initialized.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        result = qa_chain.invoke({\"query\": question})\n",
        "        answer = result[\"result\"]\n",
        "        bot.send_message(chat_id, f\"üìå Answer: {answer}\")\n",
        "        bot.send_message(chat_id,\"Send ask_command if you have any questions regarding the meeting.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.send_message(chat_id, f\"‚ùå Error while answering: {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "bot.polling()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vaow78coE9vc",
        "outputId": "313e269b-84e4-400c-b9cd-4bf2a0463d9a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé• Processing YouTube video...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbj-rCVCcfkZ"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}